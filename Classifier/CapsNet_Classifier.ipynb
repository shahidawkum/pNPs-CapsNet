{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Bu_5hst1sFCT"
      },
      "outputs": [],
      "source": [
        "## Import necessary modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score,KFold,cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7hPP8tAisNa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/NP/MRMD_BTG/Nps-train_features.csv')"
      ],
      "metadata": {
        "id": "CVpWibRqsQSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Initialise the Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# To scale data\n",
        "scaler.fit(df)"
      ],
      "metadata": {
        "id": "tuN52YK3sWsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(999, inplace=True)"
      ],
      "metadata": {
        "id": "JWXJZEqqsiWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and labels (y)\n",
        "X = df.iloc[:, 0:170].values\n",
        "y = df.iloc [:, 170].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "7bBnoqf_slz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and test set\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X, y, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ],
      "metadata": {
        "id": "tK5pd66VsnVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import layers, models, optimizers, losses, callbacks\n",
        "\n",
        "# Define a custom Capsule layer\n",
        "class CapsuleLayer(layers.Layer):\n",
        "    def __init__(self, num_capsules, dim_capsules, routings=3, **kwargs):\n",
        "        super(CapsuleLayer, self).__init__(**kwargs)\n",
        "        self.num_capsules = num_capsules\n",
        "        self.dim_capsules = dim_capsules\n",
        "        self.routings = routings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(shape=(input_shape[-1], self.num_capsules * self.dim_capsules),\n",
        "                                      initializer='glorot_uniform',\n",
        "                                      name='kernel')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs_hat = tf.linalg.matmul(inputs, self.kernel)\n",
        "        inputs_hat = tf.reshape(inputs_hat, (-1, inputs.shape[1], self.num_capsules, self.dim_capsules))\n",
        "        inputs_hat = tf.transpose(inputs_hat, perm=[0, 2, 1, 3])\n",
        "\n",
        "        b = tf.zeros_like(inputs_hat[:, :, :, 0])\n",
        "        for i in range(self.routings):\n",
        "            c = tf.nn.softmax(b, axis=1)\n",
        "            s = tf.reduce_sum(c[..., tf.newaxis] * inputs_hat, axis=2)\n",
        "            v = self.squash(s)\n",
        "            if i < self.routings - 1:\n",
        "                b += tf.reduce_sum(inputs_hat * v[:, :, tf.newaxis, :], axis=-1)\n",
        "        return v\n",
        "\n",
        "    def squash(self, s):\n",
        "        s_squared_norm = tf.reduce_sum(tf.square(s), axis=-1, keepdims=True)\n",
        "        scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
        "        return scale * s\n",
        "\n",
        "# Define the Capsule Network model\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "# Initial Conv1D layer\n",
        "x = layers.Conv1D(64, 1, activation='relu')(inputs)\n",
        "x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "# Second Conv1D layer\n",
        "x = layers.Conv1D(64, 6, activation='relu')(x)\n",
        "x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "# Third Conv1D layer\n",
        "x = layers.Conv1D(32, 1, activation='relu')(x)\n",
        "x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "# Flatten layer\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "# Primary Capsule layer\n",
        "x = layers.Dense(128)(x)\n",
        "x = layers.Reshape((-1, 8))(x)\n",
        "\n",
        "# Capsule layer\n",
        "capsules = CapsuleLayer(num_capsules=10, dim_capsules=20)(x)\n",
        "\n",
        "# Output layer\n",
        "capsules = layers.Flatten()(capsules)\n",
        "outputs = layers.Dense(1, activation='sigmoid')(capsules)\n",
        "\n",
        "model_capsnet = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_capsnet.compile(optimizer=optimizers.RMSprop(),\n",
        "                      loss=losses.BinaryCrossentropy(),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "model_capsnet.summary()\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "accuracy_list = []\n",
        "f1_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "sensitivity_list = []\n",
        "specificity_list = []\n",
        "mcc_list = []\n",
        "auc_list = []  # Initialize a list for AUC values\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    history=model_capsnet.fit(X_train_fold, y_train_fold, epochs=40, batch_size=64, verbose=0,validation_split=0.1)\n",
        "\n",
        "    # Evaluate the model on the validation set for the current fold\n",
        "    y_val_pred = model_capsnet.predict(X_val_fold)\n",
        "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate confusion matrix for the current fold\n",
        "    cm = confusion_matrix(y_val_fold, y_val_pred_binary)\n",
        "    # Calculate AUC for the current fold\n",
        "    auc = roc_auc_score(y_val_fold, y_val_pred)  # Use predicted probabilities for AUC calculation\n",
        "    auc_list.append(auc)\n",
        "\n",
        "\n",
        "    # Calculate performance evaluation metrics for the current fold\n",
        "    TN = cm[0, 0]\n",
        "    FP = cm[0, 1]\n",
        "    FN = cm[1, 0]\n",
        "    TP = cm[1, 1]\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    # F1-Score\n",
        "    f1 = 2 * TP / float(2 * TP + FP + FN)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Precision\n",
        "    precision = TP / float(TP + FP)\n",
        "    precision_list.append(precision)\n",
        "\n",
        "    # Recall\n",
        "    recall = TP / float(TP + FN)\n",
        "    recall_list.append(recall)\n",
        "\n",
        "    # Sensitivity\n",
        "    sensitivity = TP / float(TP + FN)\n",
        "    sensitivity_list.append(sensitivity)\n",
        "\n",
        "    # Specificity\n",
        "    specificity = TN / float(TN + FP)\n",
        "    specificity_list.append(specificity)\n",
        "\n",
        "    # MCC (Matthews Correlation Coefficient)\n",
        "    mcc = ((TP * TN) - (FP * FN)) / float((np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))) or 1)\n",
        "    mcc_list.append(mcc)\n",
        "\n",
        "# Calculate average performance evaluation metrics across all folds\n",
        "avg_accuracy = np.mean(accuracy_list)\n",
        "avg_f1 = np.mean(f1_list)\n",
        "avg_precision = np.mean(precision_list)\n",
        "avg_recall = np.mean(recall_list)\n",
        "avg_sensitivity = np.mean(sensitivity_list)\n",
        "avg_specificity = np.mean(specificity_list)\n",
        "avg_mcc = np.mean(mcc_list)\n",
        "\n",
        "# Calculate average AUC across all folds\n",
        "avg_auc = np.mean(auc_list)\n",
        "# Print average performance evaluation metrics\n",
        "print(\"Accuracy =\", avg_accuracy)\n",
        "print(\"F1 Score =\", avg_f1)\n",
        "print(\"Precision =\", avg_precision)\n",
        "print(\"Recall =\", avg_recall)\n",
        "print(\"Sensitivity =\", avg_sensitivity)\n",
        "print(\"Specificity =\", avg_specificity)\n",
        "print(\"MCC =\", avg_mcc)\n",
        "print(\"AUC =\", avg_auc)\n"
      ],
      "metadata": {
        "id": "tZqrPj5GsnzQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}